# ğŸ“„ PDF Chat using GenAI (RAG Application)

An end-to-end **Generative AI application** that allows users to upload a PDF and ask questions based strictly on the document content using **Retrieval Augmented Generation (RAG)**.

Built with **LangChain (LCEL)**, **Gemini LLM**, **Hugging Face Embeddings**, **FAISS**, and a clean **Streamlit UI**.

---

## ğŸš€ Live Features

- ğŸ“¤ Upload any PDF document  
- ğŸ’¬ Ask natural language questions  
- ğŸ§  Context-aware answers (no hallucination)  
- âš¡ Fast semantic search using vector embeddings  
- ğŸ–¥ï¸ Clean UI (no custom CSS)  
- ğŸ”’ Secure API key handling via `.env`

---

## ğŸ§  How It Works (RAG Pipeline)

**English:**
1. PDF is loaded and split into chunks  
2. Chunks are converted into embeddings  
3. Embeddings are stored in FAISS (vector database)  
4. User query is embedded and matched semantically  
5. Relevant chunks are passed to Gemini LLM  
6. LLM answers strictly from retrieved context  


## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-----|-----------|
| Language | Python |
| LLM | Gemini (Free tier) |
| Framework | LangChain (LCEL) |
| Embeddings | Hugging Face |
| Vector DB | FAISS |
| UI | Streamlit |
| Env Management | python-dotenv |
| Version Control | Git & GitHub |

---

## ğŸ“ Project Structure
pypdf_llm/
â”‚
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ pdf_loader.py # PDF loading & chunking
â”œâ”€â”€ vector_store.py # Embeddings & FAISS
â”œâ”€â”€ llm_chain.py # LCEL-based RAG chain
â”œâ”€â”€ config.py # Environment config
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

